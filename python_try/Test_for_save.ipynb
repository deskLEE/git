{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:71: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Functions for downloading and reading MNIST data.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import gzip\n",
    "import os\n",
    "import tensorflow.python.platform\n",
    "import numpy\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\n",
    "def maybe_download(filename, work_directory):\n",
    "  \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n",
    "  if not os.path.exists(work_directory):\n",
    "    os.mkdir(work_directory)\n",
    "  filepath = os.path.join(work_directory, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "  return filepath\n",
    "def _read32(bytestream):\n",
    "  dt = numpy.dtype(numpy.uint32).newbyteorder('>')\n",
    "  return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n",
    "# _read32函数：\n",
    "# dt为 int 32位数据类型 且bit按照大端排序 的数据格式\n",
    "# 返回值为 从bytestream中读取4个字节，即32位， 且格式为 dt 且取[0]\n",
    "def extract_images(filename):\n",
    "  \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    magic = _read32(bytestream) #第一个字节为magic值\n",
    "    if magic != 2051:\n",
    "      raise ValueError(\n",
    "          'Invalid magic number %d in MNIST image file: %s' %\n",
    "          (magic, filename))\n",
    "    num_images = _read32(bytestream) #第二个字节为图片数目 为60000\n",
    "    rows = _read32(bytestream) #第三个字节为图片的行数  为28\n",
    "    cols = _read32(bytestream) #第四个字节为图片的列数  为28\n",
    "    buf = bytestream.read(rows * cols * num_images)  # buf为读出图片的bit流\n",
    "    data = numpy.frombuffer(buf, dtype=numpy.uint8)  # 将buf流转为int8 格式的数据流\n",
    "    data = data.reshape(num_images, rows, cols, 1)   # 将data 全部改为(60000*28*28*1) 的size\n",
    "    return data   #返回\n",
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "  \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "  num_labels = labels_dense.shape[0]\n",
    "  index_offset = numpy.arange(num_labels) * num_classes\n",
    "  labels_one_hot = numpy.zeros((num_labels, num_classes))\n",
    "  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "  return labels_one_hot\n",
    "# dense_to_one_hot 函数\n",
    "# num_labels为labels的个数  index_offset 为扩大num_classes倍数 labels_one_hot 为(60000,10)\n",
    "# 返回的是(60000,10)的labels\n",
    "def extract_labels(filename, one_hot=False):  #提取labels的函数 extract_labels\n",
    "  \"\"\"Extract the labels into a 1D uint8 numpy array [index].\"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    magic = _read32(bytestream)\n",
    "    if magic != 2049:\n",
    "      raise ValueError(\n",
    "          'Invalid magic number %d in MNIST label file: %s' %\n",
    "          (magic, filename))\n",
    "    num_items = _read32(bytestream)   # num_items 为 60000\n",
    "    buf = bytestream.read(num_items)  # buf为数据\n",
    "    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "    if one_hot:  #若one_hot为true\n",
    "      return dense_to_one_hot(labels)   #返回one_hot编码\n",
    "    return labels  # 返回提取出来的编码\n",
    "class DataSet(object):\n",
    "  def __init__(self, images, labels, fake_data=False, one_hot=False,\n",
    "               dtype=tf.float32):\n",
    "    \"\"\"Construct a DataSet.\n",
    "    one_hot arg is used only if fake_data is true.  `dtype` can be either\n",
    "    `uint8` to leave the input as `[0, 255]`, or `float32` to rescale into\n",
    "    `[0, 1]`.\n",
    "    \"\"\"\n",
    "    dtype = tf.as_dtype(dtype).base_dtype\n",
    "    # dtype 是base_dtype格式\n",
    "    if dtype not in (tf.uint8, tf.float32):\n",
    "      raise TypeError('Invalid image dtype %r, expected uint8 or float32' %\n",
    "                      dtype)\n",
    "    # 如果dtype 不是int8 或者是 float32\n",
    "    if fake_data:\n",
    "      self._num_examples = 10000\n",
    "      self.one_hot = one_hot\n",
    "    # 如果fake_data是True，那么 定义两个变量 _num_examples = 10000, one_hot = one_hot的值\n",
    "    else:\n",
    "      assert images.shape[0] == labels.shape[0], (\n",
    "          'images.shape: %s labels.shape: %s' % (images.shape,\n",
    "                                                 labels.shape))\n",
    "      self._num_examples = images.shape[0]\n",
    "    # 否则， 让images.shape[0] = labels.shape[0]\n",
    "    # 令 _num_examples = images.shape[0]\n",
    "      # Convert shape from [num examples, rows, columns, depth]\n",
    "      # to [num examples, rows*columns] (assuming depth == 1)\n",
    "      assert images.shape[3] == 1\n",
    "      images = images.reshape(images.shape[0],\n",
    "                              images.shape[1] * images.shape[2])\n",
    "    # reshape images   (num*(size*size))\n",
    "      if dtype == tf.float32:\n",
    "        # Convert from [0, 255] -> [0.0, 1.0].\n",
    "        images = images.astype(numpy.float32)\n",
    "        images = numpy.multiply(images, 1.0 / 255.0)\n",
    "    # 若dtype 为 float32，那么 images就全部归一化\n",
    "    self._images = images\n",
    "    # 定义_images = images  _labels = labels _epochs_completed = 0 _index_in_epoch = 0\n",
    "    self._labels = labels\n",
    "    self._epochs_completed = 0\n",
    "    self._index_in_epoch = 0\n",
    "  @property\n",
    "  def images(self):\n",
    "    return self._images  #property装饰器\n",
    "  # self._images 的值可以通过Dataset.images 获得。\n",
    "  @property\n",
    "  def labels(self):\n",
    "    return self._labels\n",
    "  @property\n",
    "  def num_examples(self):\n",
    "    return self._num_examples\n",
    "  @property\n",
    "  def epochs_completed(self):\n",
    "    return self._epochs_completed\n",
    "  # 其余同上\n",
    "  def next_batch(self, batch_size, fake_data=False):\n",
    "  # 定义一个函数 next_batch\n",
    "    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "    if fake_data:\n",
    "      fake_image = [1] * 784\n",
    "      if self.one_hot:\n",
    "        fake_label = [1] + [0] * 9\n",
    "      else:\n",
    "        fake_label = 0\n",
    "      return [fake_image for _ in xrange(batch_size)], [\n",
    "          fake_label for _ in xrange(batch_size)]\n",
    "    start = self._index_in_epoch\n",
    "    self._index_in_epoch += batch_size\n",
    "  # _index_in_epoch 等于 原来值+batch_size\n",
    "    if self._index_in_epoch > self._num_examples:\n",
    "  # 若 _index_in_epoch大于总图片数目\n",
    "      # Finished epoch\n",
    "      self._epochs_completed += 1\n",
    "  # _epochs_completed + 1\n",
    "      # Shuffle the data\n",
    "      perm = numpy.arange(self._num_examples)\n",
    "  # prem 为[0,1,...,59999]\n",
    "      numpy.random.shuffle(perm)\n",
    "  # 打乱perm数组的排序\n",
    "      self._images = self._images[perm]\n",
    "  # 将图片按照perm数组的编号排序\n",
    "      self._labels = self._labels[perm]\n",
    "  # labels同上\n",
    "      # Start next epoch\n",
    "      start = 0\n",
    "      self._index_in_epoch = batch_size\n",
    "      assert batch_size <= self._num_examples\n",
    "    end = self._index_in_epoch\n",
    "  # end = index_in_epoch\n",
    "    return self._images[start:end], self._labels[start:end]\n",
    "  # 返回images 和 labels 的[0,batch_size] 部分\n",
    "def read_data_sets(train_dir, fake_data=False, one_hot=False, dtype=tf.float32):\n",
    "  class DataSets(object):\n",
    "    pass            # 定义一个class DataSets\n",
    "  data_sets = DataSets()   # 实例化这个class， 并命名为data_sets\n",
    "  if fake_data:     # 若fake_data是true，那么\n",
    "    def fake():\n",
    "      return DataSet([], [], fake_data=True, one_hot=one_hot, dtype=dtype)   #定义fake函数\n",
    "    data_sets.train = fake()    # data_sets的train方法 是fake函数\n",
    "    data_sets.validation = fake()  # data_sets的validation方法 是fake函数\n",
    "    data_sets.test = fake()     # data_sets的test方法 是fake函数\n",
    "    return data_sets            # read_data_sets 函数返回实例 data_sets\n",
    "  # 若fake_data是false，那么\n",
    "  TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n",
    "  TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n",
    "  TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
    "  TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
    "  VALIDATION_SIZE = 5000\n",
    "  local_file = maybe_download(TRAIN_IMAGES, train_dir)   # 定义一个local_file   为maybe_download函数的返回值\n",
    "  # def maybe_download:\n",
    "  # 返回值为filepath\n",
    "  train_images = extract_images(local_file)  # train_images 为extract_images 的返回值\n",
    "  # def extract_images:\n",
    "  # 返回值为图片bit，且格式为(num*size*size*1)\n",
    "  local_file = maybe_download(TRAIN_LABELS, train_dir)\n",
    "  train_labels = extract_labels(local_file, one_hot=one_hot)\n",
    "  # one_hot默认值为false，返回图片的label，且格式为(num*class)\n",
    "  local_file = maybe_download(TEST_IMAGES, train_dir)\n",
    "  # 同理有测试集，测试集的数据num 为10000\n",
    "  test_images = extract_images(local_file)\n",
    "  local_file = maybe_download(TEST_LABELS, train_dir)\n",
    "  test_labels = extract_labels(local_file, one_hot=one_hot)\n",
    "  validation_images = train_images[:VALIDATION_SIZE]\n",
    "  # 提出一部分作为验证集，其中验证集的size为5000\n",
    "  validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "  train_images = train_images[VALIDATION_SIZE:]\n",
    "  # 剩余的部分为训练集\n",
    "  train_labels = train_labels[VALIDATION_SIZE:]\n",
    "  data_sets.train = DataSet(train_images, train_labels, dtype=dtype)\n",
    "  # dtype 为32位的浮点数  DataSet实例化 为 data_sets.train\n",
    "  data_sets.validation = DataSet(validation_images, validation_labels,\n",
    "                                 dtype=dtype)\n",
    "  # DataSet 实例化为 data_sets.validation\n",
    "  data_sets.test = DataSet(test_images, test_labels, dtype=dtype)\n",
    "  # DataSet 实例化为 data_sets.test\n",
    "  return data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "from numpy import random\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(5000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "compression_nodenum = 784\n",
    "ph1 = 28\n",
    "ph2 = 28\n",
    "obmatrix = 1 * np.random.randn(784, compression_nodenum) + 0\n",
    "# 将数据全部整理出来变成新的数据集\n",
    "input1_raw = mnist.train.images\n",
    "valida1_raw = mnist.validation.images\n",
    "test1_raw = mnist.test.images\n",
    "# 原始数据集的数据\n",
    "input1_compression = np.matmul(input1_raw, obmatrix)\n",
    "valida1_compression = np.matmul(valida1_raw, obmatrix)\n",
    "test1_compression = np.matmul(test1_raw, obmatrix)\n",
    "# 压缩完成后的数据\n",
    "input1_shape = np.pad(input1_compression, ((0, 0), (0, ph1 * ph2 - compression_nodenum)), 'constant',\n",
    "                      constant_values=(0))\n",
    "valida1_shape = np.pad(valida1_compression, ((0, 0), (0, ph1 * ph2 - compression_nodenum)), 'constant',\n",
    "                       constant_values=(0))\n",
    "test1_shape = np.pad(test1_compression, ((0, 0), (0, ph1 * ph2 - compression_nodenum)), 'constant', constant_values=(0))\n",
    "# 重新resize了一下\n",
    "# 获得了包含压缩信息的新的数据集数据\n",
    "print(input1_shape.shape)\n",
    "print(valida1_shape.shape)\n",
    "print(test1_shape.shape)\n",
    "# input1_add.shape   (55000, 20, 20)\n",
    "# valida1_add.shape  (5000, 20, 20)\n",
    "# test1_add.shape    (10000, 20, 20)\n",
    "\n",
    "# 将数据集全部打散重新形成新的数据集\n",
    "perm_input2 = np.arange(valida1_shape.shape[0])\n",
    "np.random.shuffle(perm_input2)\n",
    "valida1_image = valida1_shape[perm_input2]\n",
    "valida1_label = mnist.validation.labels[perm_input2]\n",
    "# 获得validation数据集的格式\n",
    "perm_input3 = np.arange(test1_shape.shape[0])\n",
    "np.random.shuffle(perm_input3)\n",
    "test1_image = test1_shape[perm_input3]\n",
    "test1_label = mnist.test.labels[perm_input3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28, 32)\n",
      "(?, 14, 14, 32)\n",
      "WARNING:tensorflow:From <ipython-input-5-65944afe7db5>:49: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-5-65944afe7db5>:60: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = tf.placeholder(tf.int32, [])  # 注意类型必须为 tf.int32\n",
    "# 在 1.0 版本以后请使用 ：\n",
    "# keep_prob = tf.placeholder(tf.float32, [])\n",
    "# batch_size = tf.placeholder(tf.int32, [])\n",
    "x = tf.placeholder(tf.float32, shape=[None, ph1*ph2], name=\"x\")\n",
    "# x的大小为[batch_size*784]\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y_\")\n",
    "# y的大小为[batch_size*10]\n",
    "x_image = tf.reshape(x, [-1, ph1, ph2, 1])\n",
    "\n",
    "# 网络参数\n",
    "filter1_size = 5\n",
    "filter2_size = 5\n",
    "W_conv1 = tf.Variable(tf.truncated_normal([filter1_size, filter1_size, 1, 32], stddev=0.1), name='W_conv1')\n",
    "# W_conv1 产生正太分布随机数，标准差为0.1， shape 为[size*size*depth*num] 卷积核个数为32个\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[32]), name='b_conv1')\n",
    "# b_con1 为常数(num,)\n",
    "L1_conv = tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# 第一层做卷积,步长为1\n",
    "print(L1_conv.shape)\n",
    "# L1_conv的大小为(batch_size*28*28*num)\n",
    "L1_relu = tf.nn.relu(L1_conv + b_conv1)\n",
    "# 第一层做非线形，使用的是RELU\n",
    "L1_pool = tf.nn.max_pool(L1_relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "# ksize 为池化窗口大小 [2,2] strides 为[2,2]\n",
    "# L1_pool的大小为(batch_size*14*14*num)\n",
    "print(L1_pool.shape)\n",
    "# 定义第二个卷积层的variables和ops\n",
    "W_conv2 = tf.Variable(tf.truncated_normal([filter2_size, filter2_size, 32, 32], stddev=0.1), name='W_conv2')\n",
    "# W_conv2 shape 为[size*size*depth*num]\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[32]), name='b_conv2')\n",
    "# 生成偏置\n",
    "L2_conv = tf.nn.conv2d(L1_pool, W_conv2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# 做卷积 shape为(batch_size*14*14*num)\n",
    "L2_relu = tf.nn.relu(L2_conv + b_conv2)\n",
    "L2_pool = tf.nn.max_pool(L2_relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "# 池化层的shape 为 (batch_size*7*7*num)\n",
    "# 全连接层\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([math.ceil(ph1/4)* math.ceil(ph2/4) * 32, 512], stddev=0.1), name='W_fc1')\n",
    "# 转为（512）\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape=[512]), name='b_fc1')\n",
    "\n",
    "h_pool2_flat = tf.reshape(L2_pool, [-1, math.ceil(ph1/4)* math.ceil(ph2/4) * 32])\n",
    "# h_pool2_flat的输出shape为(batch_size,7*7*32)\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "# 输出h_fc1的shape为(batch_size,512)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "# dropout层 输出结果为 h_fc1_drop\n",
    "\n",
    "# readout层\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([512, 10], stddev=0.1), name='W_fc2')\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]), name='b_fc2')\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "# y_conv为结果   若重复出结果，就需要这个y_conv\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "# 交叉熵设为 softmax_cross_entropy\n",
    "train_step = tf.train.AdamOptimizer((1e-4)).minimize(cross_entropy)\n",
    "# 采用Adam的反向传播\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "# 若相等，则加1\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "# 求平均值即为准确率\n",
    "# 代码持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()  # 保存模型的saver函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据训练集分成 430 批, 前面每批 128 个数据，最后一批 88 个数据\n",
      "1-iteration 0: validation_accuracy 0.7692\n",
      "2-iteration 0: test_accuracy 0.7665\n",
      "1-iteration 5: validation_accuracy 0.896\n",
      "2-iteration 5: test_accuracy 0.8941\n",
      "1-iteration 10: validation_accuracy 0.9322\n",
      "2-iteration 10: test_accuracy 0.9304\n",
      "1-iteration 15: validation_accuracy 0.9498\n",
      "2-iteration 15: test_accuracy 0.9459\n",
      "[[ 958    0    4    2    0    5    7    2    2    0]\n",
      " [   0 1121    3    2    0    0    3    1    5    0]\n",
      " [   9    1  971   14    2    1    5   15   12    2]\n",
      " [   1    0    6  966    0   12    2    8   13    2]\n",
      " [   0    1    6    0  938    2    7    1    5   22]\n",
      " [   4    0    2   20    4  836    4    2   12    8]\n",
      " [   7    5    3    0    6   10  920    2    5    0]\n",
      " [   0    9   14    4    4    0    0  974    1   22]\n",
      " [   4    0    7   16    6   14    5    5  909    8]\n",
      " [   8    5    1    9   24    3    1   12   11  935]]\n",
      "save the model\n",
      "完成训练!\n",
      "834922\n"
     ]
    }
   ],
   "source": [
    "def count2():\n",
    "    print (np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]))\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "#sess = tf.Session(config=config)\n",
    "input_count=55000 \n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 初始化各个variables\n",
    "    # print (\"一共读取了 %s 个测试图像， %s 个标签\" % (input_count, input_count))\n",
    "\n",
    "    # 设置每次训练op的输入个数和迭代次数，这里为了支持任意图片总数，定义了一个余数remainder，譬如，如果每次训练op的输入个数为60，图片总数为150张，则前面两次各输入60张，最后一次输入30张（余数30）\n",
    "    batch_size = 128\n",
    "    iterations = 20   # 先用迭代的20次看一下结果\n",
    "    train_num = 55000\n",
    "\n",
    "    batches_count = int(train_num / batch_size)  # 可以分为多少批\n",
    "    remainder = train_num % batch_size\n",
    "    print(\"数据训练集分成 %s 批, 前面每批 %s 个数据，最后一批 %s 个数据\" % (batches_count + 1, batch_size, remainder))\n",
    "    # 数据集分成batches_count+1 批\n",
    "\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # 在每一次训练之后都需要一个shuffle的过程，大概可以提高训练的结果准确率\n",
    "        perm_input1 = np.arange(input1_shape.shape[0])\n",
    "        # prem 为[0,1,...,59999]\n",
    "        np.random.shuffle(perm_input1)\n",
    "        # 打乱perm数组的排序\n",
    "        input1_image = input1_shape[perm_input1]\n",
    "        input1_label = mnist.train.labels[perm_input1]\n",
    "            # 获得train数据集的格式\n",
    "        for n in range(batches_count):\n",
    "            train_step.run(feed_dict={x: input1_image[n * batch_size:(n + 1) * batch_size],\n",
    "                                      y_: input1_label[n * batch_size:(n + 1) * batch_size], keep_prob: 0.5})\n",
    "        # 每次抽出128个进行训练，训练整个数据集\n",
    "        # 如果remainder > 0，那么就将结果放进去\n",
    "        # 将placeholder换掉，使用input1和mnist.train.labels去替代x与y_\n",
    "        # 验证集同上进行训练\n",
    "        if remainder > 0:\n",
    "            start_index = batches_count * batch_size;\n",
    "            train_step.run(\n",
    "                feed_dict={x: input1_image[start_index:input_count - 1],\n",
    "                           y_: input1_label[start_index:input_count - 1],\n",
    "                           keep_prob: 0.5})    # 这个网络有dropout层\n",
    "# 测试集\n",
    "        iterate_accuracy = 0\n",
    "        validation_accuracy = 0\n",
    "        if i % 5 == 0:\n",
    "            validation_accuracy = accuracy.eval(feed_dict={x : valida1_image, y_: valida1_label, keep_prob: 1.0})\n",
    "            # 使用验证集进行验证\n",
    "            iterate_accuracy = accuracy.eval(feed_dict={x: test1_image, y_: test1_label, keep_prob: 1.0})\n",
    "                        # 使用验证集进行验证\n",
    "            print('1-iteration %d: validation_accuracy %s' % (i, validation_accuracy))\n",
    "            print('2-iteration %d: test_accuracy %s' % (i, iterate_accuracy))\n",
    "    y_pred = y_conv.eval(feed_dict={x: test1_image, y_: test1_label, keep_prob: 1.0})\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(test1_label, axis=1)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "    saver.save(sess, \"./tmp/model.ckpt\")    # 利用saver.save 函数保存model的ckpt文件\n",
    "    print (\"save the model\")\n",
    "    print('完成训练!')\n",
    "\n",
    "count2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W_conv1:0', 'b_conv1:0', 'W_conv2:0', 'b_conv2:0', 'W_fc1:0', 'b_fc1:0', 'W_fc2:0', 'b_fc2:0']\n"
     ]
    }
   ],
   "source": [
    "variable_names = [v.name for v in tf.trainable_variables()]\n",
    "print(variable_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder \n",
      "\n",
      "x \n",
      "\n",
      "y_ \n",
      "\n",
      "Reshape/shape \n",
      "\n",
      "Reshape \n",
      "\n",
      "truncated_normal/shape \n",
      "\n",
      "truncated_normal/mean \n",
      "\n",
      "truncated_normal/stddev \n",
      "\n",
      "truncated_normal/TruncatedNormal \n",
      "\n",
      "truncated_normal/mul \n",
      "\n",
      "truncated_normal \n",
      "\n",
      "W_conv1 \n",
      "\n",
      "W_conv1/Assign \n",
      "\n",
      "W_conv1/read \n",
      "\n",
      "Const \n",
      "\n",
      "b_conv1 \n",
      "\n",
      "b_conv1/Assign \n",
      "\n",
      "b_conv1/read \n",
      "\n",
      "Conv2D \n",
      "\n",
      "add \n",
      "\n",
      "Relu \n",
      "\n",
      "MaxPool \n",
      "\n",
      "truncated_normal_1/shape \n",
      "\n",
      "truncated_normal_1/mean \n",
      "\n",
      "truncated_normal_1/stddev \n",
      "\n",
      "truncated_normal_1/TruncatedNormal \n",
      "\n",
      "truncated_normal_1/mul \n",
      "\n",
      "truncated_normal_1 \n",
      "\n",
      "W_conv2 \n",
      "\n",
      "W_conv2/Assign \n",
      "\n",
      "W_conv2/read \n",
      "\n",
      "Const_1 \n",
      "\n",
      "b_conv2 \n",
      "\n",
      "b_conv2/Assign \n",
      "\n",
      "b_conv2/read \n",
      "\n",
      "Conv2D_1 \n",
      "\n",
      "add_1 \n",
      "\n",
      "Relu_1 \n",
      "\n",
      "MaxPool_1 \n",
      "\n",
      "truncated_normal_2/shape \n",
      "\n",
      "truncated_normal_2/mean \n",
      "\n",
      "truncated_normal_2/stddev \n",
      "\n",
      "truncated_normal_2/TruncatedNormal \n",
      "\n",
      "truncated_normal_2/mul \n",
      "\n",
      "truncated_normal_2 \n",
      "\n",
      "W_fc1 \n",
      "\n",
      "W_fc1/Assign \n",
      "\n",
      "W_fc1/read \n",
      "\n",
      "Const_2 \n",
      "\n",
      "b_fc1 \n",
      "\n",
      "b_fc1/Assign \n",
      "\n",
      "b_fc1/read \n",
      "\n",
      "Reshape_1/shape \n",
      "\n",
      "Reshape_1 \n",
      "\n",
      "MatMul \n",
      "\n",
      "add_2 \n",
      "\n",
      "Relu_2 \n",
      "\n",
      "keep_prob \n",
      "\n",
      "sub/x \n",
      "\n",
      "sub \n",
      "\n",
      "dropout/Shape \n",
      "\n",
      "dropout/random_uniform/min \n",
      "\n",
      "dropout/random_uniform/max \n",
      "\n",
      "dropout/random_uniform/RandomUniform \n",
      "\n",
      "dropout/random_uniform/sub \n",
      "\n",
      "dropout/random_uniform/mul \n",
      "\n",
      "dropout/random_uniform \n",
      "\n",
      "dropout/sub/x \n",
      "\n",
      "dropout/sub \n",
      "\n",
      "dropout/truediv/x \n",
      "\n",
      "dropout/truediv \n",
      "\n",
      "dropout/GreaterEqual \n",
      "\n",
      "dropout/mul \n",
      "\n",
      "dropout/Cast \n",
      "\n",
      "dropout/mul_1 \n",
      "\n",
      "truncated_normal_3/shape \n",
      "\n",
      "truncated_normal_3/mean \n",
      "\n",
      "truncated_normal_3/stddev \n",
      "\n",
      "truncated_normal_3/TruncatedNormal \n",
      "\n",
      "truncated_normal_3/mul \n",
      "\n",
      "truncated_normal_3 \n",
      "\n",
      "W_fc2 \n",
      "\n",
      "W_fc2/Assign \n",
      "\n",
      "W_fc2/read \n",
      "\n",
      "Const_3 \n",
      "\n",
      "b_fc2 \n",
      "\n",
      "b_fc2/Assign \n",
      "\n",
      "b_fc2/read \n",
      "\n",
      "MatMul_1 \n",
      "\n",
      "add_3 \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/labels_stop_gradient \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Rank \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Shape \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Rank_1 \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Shape_1 \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Sub/y \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Sub \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Slice/begin \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Slice/size \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Slice \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/concat/values_0 \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/concat/axis \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/concat \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Reshape \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Rank_2 \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Shape_2 \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Sub_1/y \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Sub_1 \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Slice_1/begin \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Slice_1/size \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Slice_1 \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/concat_1/values_0 \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/concat_1/axis \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/concat_1 \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Reshape_1 \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Sub_2/y \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Sub_2 \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Slice_2/begin \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Slice_2/size \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Slice_2 \n",
      "\n",
      "softmax_cross_entropy_with_logits_sg/Reshape_2 \n",
      "\n",
      "Const_4 \n",
      "\n",
      "Mean \n",
      "\n",
      "gradients/Shape \n",
      "\n",
      "gradients/grad_ys_0 \n",
      "\n",
      "gradients/Fill \n",
      "\n",
      "gradients/Mean_grad/Reshape/shape \n",
      "\n",
      "gradients/Mean_grad/Reshape \n",
      "\n",
      "gradients/Mean_grad/Shape \n",
      "\n",
      "gradients/Mean_grad/Tile \n",
      "\n",
      "gradients/Mean_grad/Shape_1 \n",
      "\n",
      "gradients/Mean_grad/Shape_2 \n",
      "\n",
      "gradients/Mean_grad/Const \n",
      "\n",
      "gradients/Mean_grad/Prod \n",
      "\n",
      "gradients/Mean_grad/Const_1 \n",
      "\n",
      "gradients/Mean_grad/Prod_1 \n",
      "\n",
      "gradients/Mean_grad/Maximum/y \n",
      "\n",
      "gradients/Mean_grad/Maximum \n",
      "\n",
      "gradients/Mean_grad/floordiv \n",
      "\n",
      "gradients/Mean_grad/Cast \n",
      "\n",
      "gradients/Mean_grad/truediv \n",
      "\n",
      "gradients/softmax_cross_entropy_with_logits_sg/Reshape_2_grad/Shape \n",
      "\n",
      "gradients/softmax_cross_entropy_with_logits_sg/Reshape_2_grad/Reshape \n",
      "\n",
      "gradients/zeros_like \n",
      "\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims/dim \n",
      "\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims \n",
      "\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/mul \n",
      "\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/LogSoftmax \n",
      "\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/Neg \n",
      "\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1/dim \n",
      "\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1 \n",
      "\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/mul_1 \n",
      "\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/group_deps \n",
      "\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency \n",
      "\n",
      "gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency_1 \n",
      "\n",
      "gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Shape \n",
      "\n",
      "gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape \n",
      "\n",
      "gradients/add_3_grad/Shape \n",
      "\n",
      "gradients/add_3_grad/Shape_1 \n",
      "\n",
      "gradients/add_3_grad/BroadcastGradientArgs \n",
      "\n",
      "gradients/add_3_grad/Sum \n",
      "\n",
      "gradients/add_3_grad/Reshape \n",
      "\n",
      "gradients/add_3_grad/Sum_1 \n",
      "\n",
      "gradients/add_3_grad/Reshape_1 \n",
      "\n",
      "gradients/add_3_grad/tuple/group_deps \n",
      "\n",
      "gradients/add_3_grad/tuple/control_dependency \n",
      "\n",
      "gradients/add_3_grad/tuple/control_dependency_1 \n",
      "\n",
      "gradients/MatMul_1_grad/MatMul \n",
      "\n",
      "gradients/MatMul_1_grad/MatMul_1 \n",
      "\n",
      "gradients/MatMul_1_grad/tuple/group_deps \n",
      "\n",
      "gradients/MatMul_1_grad/tuple/control_dependency \n",
      "\n",
      "gradients/MatMul_1_grad/tuple/control_dependency_1 \n",
      "\n",
      "gradients/dropout/mul_1_grad/Shape \n",
      "\n",
      "gradients/dropout/mul_1_grad/Shape_1 \n",
      "\n",
      "gradients/dropout/mul_1_grad/BroadcastGradientArgs \n",
      "\n",
      "gradients/dropout/mul_1_grad/Mul \n",
      "\n",
      "gradients/dropout/mul_1_grad/Sum \n",
      "\n",
      "gradients/dropout/mul_1_grad/Reshape \n",
      "\n",
      "gradients/dropout/mul_1_grad/Mul_1 \n",
      "\n",
      "gradients/dropout/mul_1_grad/Sum_1 \n",
      "\n",
      "gradients/dropout/mul_1_grad/Reshape_1 \n",
      "\n",
      "gradients/dropout/mul_1_grad/tuple/group_deps \n",
      "\n",
      "gradients/dropout/mul_1_grad/tuple/control_dependency \n",
      "\n",
      "gradients/dropout/mul_1_grad/tuple/control_dependency_1 \n",
      "\n",
      "gradients/dropout/mul_grad/Shape \n",
      "\n",
      "gradients/dropout/mul_grad/Shape_1 \n",
      "\n",
      "gradients/dropout/mul_grad/BroadcastGradientArgs \n",
      "\n",
      "gradients/dropout/mul_grad/Mul \n",
      "\n",
      "gradients/dropout/mul_grad/Sum \n",
      "\n",
      "gradients/dropout/mul_grad/Reshape \n",
      "\n",
      "gradients/dropout/mul_grad/Mul_1 \n",
      "\n",
      "gradients/dropout/mul_grad/Sum_1 \n",
      "\n",
      "gradients/dropout/mul_grad/Reshape_1 \n",
      "\n",
      "gradients/dropout/mul_grad/tuple/group_deps \n",
      "\n",
      "gradients/dropout/mul_grad/tuple/control_dependency \n",
      "\n",
      "gradients/dropout/mul_grad/tuple/control_dependency_1 \n",
      "\n",
      "gradients/Relu_2_grad/ReluGrad \n",
      "\n",
      "gradients/add_2_grad/Shape \n",
      "\n",
      "gradients/add_2_grad/Shape_1 \n",
      "\n",
      "gradients/add_2_grad/BroadcastGradientArgs \n",
      "\n",
      "gradients/add_2_grad/Sum \n",
      "\n",
      "gradients/add_2_grad/Reshape \n",
      "\n",
      "gradients/add_2_grad/Sum_1 \n",
      "\n",
      "gradients/add_2_grad/Reshape_1 \n",
      "\n",
      "gradients/add_2_grad/tuple/group_deps \n",
      "\n",
      "gradients/add_2_grad/tuple/control_dependency \n",
      "\n",
      "gradients/add_2_grad/tuple/control_dependency_1 \n",
      "\n",
      "gradients/MatMul_grad/MatMul \n",
      "\n",
      "gradients/MatMul_grad/MatMul_1 \n",
      "\n",
      "gradients/MatMul_grad/tuple/group_deps \n",
      "\n",
      "gradients/MatMul_grad/tuple/control_dependency \n",
      "\n",
      "gradients/MatMul_grad/tuple/control_dependency_1 \n",
      "\n",
      "gradients/Reshape_1_grad/Shape \n",
      "\n",
      "gradients/Reshape_1_grad/Reshape \n",
      "\n",
      "gradients/MaxPool_1_grad/MaxPoolGrad \n",
      "\n",
      "gradients/Relu_1_grad/ReluGrad \n",
      "\n",
      "gradients/add_1_grad/Shape \n",
      "\n",
      "gradients/add_1_grad/Shape_1 \n",
      "\n",
      "gradients/add_1_grad/BroadcastGradientArgs \n",
      "\n",
      "gradients/add_1_grad/Sum \n",
      "\n",
      "gradients/add_1_grad/Reshape \n",
      "\n",
      "gradients/add_1_grad/Sum_1 \n",
      "\n",
      "gradients/add_1_grad/Reshape_1 \n",
      "\n",
      "gradients/add_1_grad/tuple/group_deps \n",
      "\n",
      "gradients/add_1_grad/tuple/control_dependency \n",
      "\n",
      "gradients/add_1_grad/tuple/control_dependency_1 \n",
      "\n",
      "gradients/Conv2D_1_grad/ShapeN \n",
      "\n",
      "gradients/Conv2D_1_grad/Conv2DBackpropInput \n",
      "\n",
      "gradients/Conv2D_1_grad/Conv2DBackpropFilter \n",
      "\n",
      "gradients/Conv2D_1_grad/tuple/group_deps \n",
      "\n",
      "gradients/Conv2D_1_grad/tuple/control_dependency \n",
      "\n",
      "gradients/Conv2D_1_grad/tuple/control_dependency_1 \n",
      "\n",
      "gradients/MaxPool_grad/MaxPoolGrad \n",
      "\n",
      "gradients/Relu_grad/ReluGrad \n",
      "\n",
      "gradients/add_grad/Shape \n",
      "\n",
      "gradients/add_grad/Shape_1 \n",
      "\n",
      "gradients/add_grad/BroadcastGradientArgs \n",
      "\n",
      "gradients/add_grad/Sum \n",
      "\n",
      "gradients/add_grad/Reshape \n",
      "\n",
      "gradients/add_grad/Sum_1 \n",
      "\n",
      "gradients/add_grad/Reshape_1 \n",
      "\n",
      "gradients/add_grad/tuple/group_deps \n",
      "\n",
      "gradients/add_grad/tuple/control_dependency \n",
      "\n",
      "gradients/add_grad/tuple/control_dependency_1 \n",
      "\n",
      "gradients/Conv2D_grad/ShapeN \n",
      "\n",
      "gradients/Conv2D_grad/Conv2DBackpropInput \n",
      "\n",
      "gradients/Conv2D_grad/Conv2DBackpropFilter \n",
      "\n",
      "gradients/Conv2D_grad/tuple/group_deps \n",
      "\n",
      "gradients/Conv2D_grad/tuple/control_dependency \n",
      "\n",
      "gradients/Conv2D_grad/tuple/control_dependency_1 \n",
      "\n",
      "beta1_power/initial_value \n",
      "\n",
      "beta1_power \n",
      "\n",
      "beta1_power/Assign \n",
      "\n",
      "beta1_power/read \n",
      "\n",
      "beta2_power/initial_value \n",
      "\n",
      "beta2_power \n",
      "\n",
      "beta2_power/Assign \n",
      "\n",
      "beta2_power/read \n",
      "\n",
      "W_conv1/Adam/Initializer/zeros \n",
      "\n",
      "W_conv1/Adam \n",
      "\n",
      "W_conv1/Adam/Assign \n",
      "\n",
      "W_conv1/Adam/read \n",
      "\n",
      "W_conv1/Adam_1/Initializer/zeros \n",
      "\n",
      "W_conv1/Adam_1 \n",
      "\n",
      "W_conv1/Adam_1/Assign \n",
      "\n",
      "W_conv1/Adam_1/read \n",
      "\n",
      "b_conv1/Adam/Initializer/zeros \n",
      "\n",
      "b_conv1/Adam \n",
      "\n",
      "b_conv1/Adam/Assign \n",
      "\n",
      "b_conv1/Adam/read \n",
      "\n",
      "b_conv1/Adam_1/Initializer/zeros \n",
      "\n",
      "b_conv1/Adam_1 \n",
      "\n",
      "b_conv1/Adam_1/Assign \n",
      "\n",
      "b_conv1/Adam_1/read \n",
      "\n",
      "W_conv2/Adam/Initializer/zeros/shape_as_tensor \n",
      "\n",
      "W_conv2/Adam/Initializer/zeros/Const \n",
      "\n",
      "W_conv2/Adam/Initializer/zeros \n",
      "\n",
      "W_conv2/Adam \n",
      "\n",
      "W_conv2/Adam/Assign \n",
      "\n",
      "W_conv2/Adam/read \n",
      "\n",
      "W_conv2/Adam_1/Initializer/zeros/shape_as_tensor \n",
      "\n",
      "W_conv2/Adam_1/Initializer/zeros/Const \n",
      "\n",
      "W_conv2/Adam_1/Initializer/zeros \n",
      "\n",
      "W_conv2/Adam_1 \n",
      "\n",
      "W_conv2/Adam_1/Assign \n",
      "\n",
      "W_conv2/Adam_1/read \n",
      "\n",
      "b_conv2/Adam/Initializer/zeros \n",
      "\n",
      "b_conv2/Adam \n",
      "\n",
      "b_conv2/Adam/Assign \n",
      "\n",
      "b_conv2/Adam/read \n",
      "\n",
      "b_conv2/Adam_1/Initializer/zeros \n",
      "\n",
      "b_conv2/Adam_1 \n",
      "\n",
      "b_conv2/Adam_1/Assign \n",
      "\n",
      "b_conv2/Adam_1/read \n",
      "\n",
      "W_fc1/Adam/Initializer/zeros/shape_as_tensor \n",
      "\n",
      "W_fc1/Adam/Initializer/zeros/Const \n",
      "\n",
      "W_fc1/Adam/Initializer/zeros \n",
      "\n",
      "W_fc1/Adam \n",
      "\n",
      "W_fc1/Adam/Assign \n",
      "\n",
      "W_fc1/Adam/read \n",
      "\n",
      "W_fc1/Adam_1/Initializer/zeros/shape_as_tensor \n",
      "\n",
      "W_fc1/Adam_1/Initializer/zeros/Const \n",
      "\n",
      "W_fc1/Adam_1/Initializer/zeros \n",
      "\n",
      "W_fc1/Adam_1 \n",
      "\n",
      "W_fc1/Adam_1/Assign \n",
      "\n",
      "W_fc1/Adam_1/read \n",
      "\n",
      "b_fc1/Adam/Initializer/zeros \n",
      "\n",
      "b_fc1/Adam \n",
      "\n",
      "b_fc1/Adam/Assign \n",
      "\n",
      "b_fc1/Adam/read \n",
      "\n",
      "b_fc1/Adam_1/Initializer/zeros \n",
      "\n",
      "b_fc1/Adam_1 \n",
      "\n",
      "b_fc1/Adam_1/Assign \n",
      "\n",
      "b_fc1/Adam_1/read \n",
      "\n",
      "W_fc2/Adam/Initializer/zeros/shape_as_tensor \n",
      "\n",
      "W_fc2/Adam/Initializer/zeros/Const \n",
      "\n",
      "W_fc2/Adam/Initializer/zeros \n",
      "\n",
      "W_fc2/Adam \n",
      "\n",
      "W_fc2/Adam/Assign \n",
      "\n",
      "W_fc2/Adam/read \n",
      "\n",
      "W_fc2/Adam_1/Initializer/zeros/shape_as_tensor \n",
      "\n",
      "W_fc2/Adam_1/Initializer/zeros/Const \n",
      "\n",
      "W_fc2/Adam_1/Initializer/zeros \n",
      "\n",
      "W_fc2/Adam_1 \n",
      "\n",
      "W_fc2/Adam_1/Assign \n",
      "\n",
      "W_fc2/Adam_1/read \n",
      "\n",
      "b_fc2/Adam/Initializer/zeros \n",
      "\n",
      "b_fc2/Adam \n",
      "\n",
      "b_fc2/Adam/Assign \n",
      "\n",
      "b_fc2/Adam/read \n",
      "\n",
      "b_fc2/Adam_1/Initializer/zeros \n",
      "\n",
      "b_fc2/Adam_1 \n",
      "\n",
      "b_fc2/Adam_1/Assign \n",
      "\n",
      "b_fc2/Adam_1/read \n",
      "\n",
      "Adam/learning_rate \n",
      "\n",
      "Adam/beta1 \n",
      "\n",
      "Adam/beta2 \n",
      "\n",
      "Adam/epsilon \n",
      "\n",
      "Adam/update_W_conv1/ApplyAdam \n",
      "\n",
      "Adam/update_b_conv1/ApplyAdam \n",
      "\n",
      "Adam/update_W_conv2/ApplyAdam \n",
      "\n",
      "Adam/update_b_conv2/ApplyAdam \n",
      "\n",
      "Adam/update_W_fc1/ApplyAdam \n",
      "\n",
      "Adam/update_b_fc1/ApplyAdam \n",
      "\n",
      "Adam/update_W_fc2/ApplyAdam \n",
      "\n",
      "Adam/update_b_fc2/ApplyAdam \n",
      "\n",
      "Adam/mul \n",
      "\n",
      "Adam/Assign \n",
      "\n",
      "Adam/mul_1 \n",
      "\n",
      "Adam/Assign_1 \n",
      "\n",
      "Adam \n",
      "\n",
      "ArgMax/dimension \n",
      "\n",
      "ArgMax \n",
      "\n",
      "ArgMax_1/dimension \n",
      "\n",
      "ArgMax_1 \n",
      "\n",
      "Equal \n",
      "\n",
      "Cast \n",
      "\n",
      "Const_5 \n",
      "\n",
      "accuracy \n",
      "\n",
      "save/filename/input \n",
      "\n",
      "save/filename \n",
      "\n",
      "save/Const \n",
      "\n",
      "save/SaveV2/tensor_names \n",
      "\n",
      "save/SaveV2/shape_and_slices \n",
      "\n",
      "save/SaveV2 \n",
      "\n",
      "save/control_dependency \n",
      "\n",
      "save/RestoreV2/tensor_names \n",
      "\n",
      "save/RestoreV2/shape_and_slices \n",
      "\n",
      "save/RestoreV2 \n",
      "\n",
      "save/Assign \n",
      "\n",
      "save/Assign_1 \n",
      "\n",
      "save/Assign_2 \n",
      "\n",
      "save/Assign_3 \n",
      "\n",
      "save/Assign_4 \n",
      "\n",
      "save/Assign_5 \n",
      "\n",
      "save/Assign_6 \n",
      "\n",
      "save/Assign_7 \n",
      "\n",
      "save/Assign_8 \n",
      "\n",
      "save/Assign_9 \n",
      "\n",
      "save/Assign_10 \n",
      "\n",
      "save/Assign_11 \n",
      "\n",
      "save/Assign_12 \n",
      "\n",
      "save/Assign_13 \n",
      "\n",
      "save/Assign_14 \n",
      "\n",
      "save/Assign_15 \n",
      "\n",
      "save/Assign_16 \n",
      "\n",
      "save/Assign_17 \n",
      "\n",
      "save/Assign_18 \n",
      "\n",
      "save/Assign_19 \n",
      "\n",
      "save/Assign_20 \n",
      "\n",
      "save/Assign_21 \n",
      "\n",
      "save/Assign_22 \n",
      "\n",
      "save/Assign_23 \n",
      "\n",
      "save/Assign_24 \n",
      "\n",
      "save/Assign_25 \n",
      "\n",
      "save/restore_all \n",
      "\n",
      "init \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor_name_list = [tensor.name for tensor in tf.get_default_graph().as_graph_def().node]\n",
    "for tensor_name in tensor_name_list:\n",
    "    print(tensor_name,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从model.ckpt.meta中直接加载已经持久化的图\n",
    "saver = tf.train.import_meta_graph(\n",
    "    './tmp/model.ckpt.meta'\n",
    ")\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"x\"\n",
      "op: \"Placeholder\"\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"shape\"\n",
      "  value {\n",
      "    shape {\n",
      "      dim {\n",
      "        size: -1\n",
      "      }\n",
      "      dim {\n",
      "        size: 784\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "name: \"y_\"\n",
      "op: \"Placeholder\"\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"shape\"\n",
      "  value {\n",
      "    shape {\n",
      "      dim {\n",
      "        size: -1\n",
      "      }\n",
      "      dim {\n",
      "        size: 10\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = graph.get_operation_by_name('x')\n",
    "y_ = graph.get_operation_by_name('y_')\n",
    "print(x)\n",
    "print(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 784)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1_image[0:10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n",
      "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "[1 7]\n",
      "[1 7]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "#    saver.restore(sess, './model/model.ckpt')\n",
    "    saver.restore(sess, \"./tmp/model.ckpt\")\n",
    "    x = sess.graph.get_tensor_by_name('x:0')\n",
    "    y_ = sess.graph.get_tensor_by_name('y_:0')\n",
    "    keep_prob =sess.graph.get_tensor_by_name('keep_prob:0')\n",
    "    output = sess.graph.get_tensor_by_name('add_3:0')\n",
    "    layer1_output = sess.graph.get_tensor_by_name(\"Conv2D:0\")\n",
    "    print(layer1_output)\n",
    "    #print(sess.run(layer1_output, feed_dict={x:test1_image[0:2],keep_prob:1.0}))\n",
    "    print(sess.run(tf.argmax(output,1), feed_dict={x:valida1_image[0:2],keep_prob:1.0}))\n",
    "    print(sess.run(tf.argmax(valida1_label[0:2], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import os\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "\n",
    "checkpoint_path = os.path.join('./tmp/model.ckpt')\n",
    "reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path)\n",
    "var_to_shape_map = reader.get_variable_to_shape_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beta2_power': [], 'b_fc2': [10], 'b_fc1/Adam': [512], 'b_fc1': [512], 'b_conv2/Adam_1': [32], 'b_fc2/Adam': [10], 'b_conv2/Adam': [32], 'W_conv2/Adam': [5, 5, 32, 32], 'b_conv2': [32], 'W_fc2/Adam': [512, 10], 'W_conv2/Adam_1': [5, 5, 32, 32], 'W_conv2': [5, 5, 32, 32], 'beta1_power': [], 'W_fc2/Adam_1': [512, 10], 'W_conv1': [5, 5, 1, 32], 'W_conv1/Adam': [5, 5, 1, 32], 'b_fc2/Adam_1': [10], 'b_fc1/Adam_1': [512], 'W_fc1/Adam': [1568, 512], 'W_fc1/Adam_1': [1568, 512], 'W_fc1': [1568, 512], 'W_fc2': [512, 10], 'b_conv1': [32], 'b_conv1/Adam': [32], 'W_conv1/Adam_1': [5, 5, 1, 32], 'b_conv1/Adam_1': [32]}\n"
     ]
    }
   ],
   "source": [
    "print(var_to_shape_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 1D or 2D array, got 4D array instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e97f2fc7142e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'layer1_W.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW_conv1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%0.8f'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m             raise ValueError(\n\u001b[0;32m-> 1382\u001b[0;31m                 \"Expected 1D or 2D array, got %dD array instead\" % X.ndim)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0;31m# Common case -- 1d array of numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 1D or 2D array, got 4D array instead"
     ]
    }
   ],
   "source": [
    "np.savetxt('layer1_W.txt',W_conv1,fmt='%0.8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 1)\n",
      "(5, 5)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 1D or 2D array, got 3D array instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f8c9ee5711c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_conv1_1_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'layer1_W_1.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW_conv1_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# print(reader.get_tensor('W_conv1'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#print(W_conv1.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m             raise ValueError(\n\u001b[0;32m-> 1382\u001b[0;31m                 \"Expected 1D or 2D array, got %dD array instead\" % X.ndim)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0;31m# Common case -- 1d array of numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 1D or 2D array, got 3D array instead"
     ]
    }
   ],
   "source": [
    "W_conv1 = reader.get_tensor('W_conv1')\n",
    "W_conv1_1 = W_conv1[:,:,:,1]\n",
    "print(W_conv1_1.shape)\n",
    "W_conv1_1_1 = W_conv1_1[:,:,0]\n",
    "print(W_conv1_1_1.shape)\n",
    "\n",
    "np.savetxt('layer1_W_1.txt',W_conv1_1)\n",
    "# print(reader.get_tensor('W_conv1'))\n",
    "#print(W_conv1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
